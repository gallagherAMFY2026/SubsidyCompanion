I proposed a data shape that survives messy government inputs. The stack can be anything. Here’s the why, and how it maps cleanly to your Node/TS + Express + Postgres system so we don’t throw away what already works.

Why this structure (the core rationale)

Federated schema beats false uniformity.
Provinces/states don’t line up. Forcing one rigid table means endless exceptions. A small core + flex attributes keeps you fast without losing detail.

Docs are first-class, not attachments.
Farmers (and your bot) need a deterministic “View PDF/Web” path. A docs table with slug/hash/URL lets the UI and API treat documents like real data, not footnotes.

Single catalog for both UI and bot.
The same source should power cards, filters, and retrieval-augmented answers. One catalog avoids drift, duplicated logic, and “bot says X / UI shows Y.”

Precompute heavy stuff, keep runtime dumb.
Whether it’s PDF hashes or vector indexes, do it once. The app only reads. That’s how you stay stable on Replit (or any host).

That’s the design. SQLite in my starter kit was a reference implementation, not a mandate.

Keep your stack (Option 1). Here’s the mapping to Postgres.

You already have Node/TS + Express + Postgres + React and 282 records. Great—keep it. Port the structure, not the language.

Tables (Postgres)

programs (core fields you already have)

program_id text primary key

title text not null

country text not null

province text not null

summary text

status text -- open|rolling|deadline|closed|unknown

tags text[] -- optional; or text with delimiter if you prefer

website_url text

last_reviewed timestamptz

search_text tsvector -- for full-text search

program_attributes

program_id text references programs(program_id) on delete cascade

attr_key text

attr_value text

primary key (program_id, attr_key)

(You can also store attr_value_json jsonb if some attributes are structured.)

docs

program_id text references programs(program_id) on delete cascade

doc_id text primary key

doc_type text -- guideline|application_form|faq|checklist|terms|webpage|other|quick_guide|reference

display_name text

file_slug text -- null when webpage

source_url text -- PDF origin OR canonical webpage

language text

effective_date date

sha256 text -- for integrity if local file

notes text

Indexes

create index on programs (country, province, status);

create index on docs (program_id);

create index on program_attributes (program_id);

Full-text:

alter table programs add column search_text tsvector;

Populate with to_tsvector('english', coalesce(title,'') || ' ' || coalesce(summary,'') || ' ' || array_to_string(tags, ' '))

create index on programs using gin(search_text);

JSON/FTS equivalence

My SQLite FTS5 → your tsvector + GIN.

My attributes table → your program_attributes (or a jsonb column if you prefer programs.extras jsonb—both patterns are fine).

My JSONL export → optional view or API route that emits JSON per program (handy for the bot).

“Adapters” in Node/TS

Your import_spreadsheet_data.ts becomes the adapter hub:

Per-sheet mapping object:

const MAPS: Record<string, {title: string; summary: string; website?: string; status?: string}> = {
  "Overview of agri-food": { title: "Program Name", summary: "Description", website: "Link", status: "Status" },
  "British Columbia":     { title: "Program Name", summary: "Program Description", website: "Website", status: "Status" },
  // …repeat for each sheet
};


For each row:

Build core fields from known columns.

Everything else → program_attributes as key/value.

Generate program_id like CA-BC-{slug(title)} (keep it stable).

Docs ingestion in Node/TS

Keep your existing storage strategy:

If storing locally: static/pdfs/<originalFileName> plus sha256.

If S3: store s3_key and sha256 in docs, file_slug can still be a friendly display path.

The manifest CSV is still gold. Your Node script:

Validates each row.

If file_name present: check file exists (in ZIP or upload directory), compute sha256, upload to storage, write docs row.

If file_name blank + source_url present: write a webpage doc row.

Frontend shows “View PDF” or “View on Web” based on doc_type/file_slug.

Bot/RAG later (optional)

Emit a /programs/:id.json or /programs.jsonl endpoint (one record per program with core + attributes + docs).

If/when you embed: create a small embedding index offline and load it on boot, or use pgvector with summaries only. Not necessary to ship v1.

Why this is the right call

Preserves your React + Express + Postgres (no rewrite).

Unlocks Canada/US heterogeneity without schema wars.

Gives you clean, deterministic doc links for farmers and support teams.

Sets you up for the bot without coupling UI to LLM internals.

What to do next (concrete)

Add the three tables to Postgres (DDL above).

Augment import_spreadsheet_data.ts with per-sheet maps and an attribute sweep.

Add a docs ingestion script (Node) that reads the manifest CSV + ZIP, writes docs, computes sha256, and stores PDFs.

Frontend: on the program detail drawer, render a “Documents” list:

doc_type=webpage → button opens source_url

else → link /docs/<file_slug> or S3 URL

Search: add tsvector on programs and expose q= filter in Express route.

Answering Replit’s fork-in-the-road

Choose Option 1: Adapt new Canada/US data to your existing Node/Postgres system.
We keep all current progress, swap in the federated model, and your UI doesn’t break. The Python starter stands as a reference for how the pieces fit; we mirror it in TypeScript.

If you want, send me your import_spreadsheet_data.ts and the programs/doc tables you already have—I’ll splice in:

the per-sheet mapping pattern,

the attribute sweep,

and the docs manifest ingestion logic—so your team can “kick the tires, light some fires” without touching the frontend.